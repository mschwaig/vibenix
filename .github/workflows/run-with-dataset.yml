name: Build Test Packages

on:
  workflow_dispatch:
    inputs:
      max_packages:
        description: 'Maximum number of packages to build (0 for all)'
        required: false
        default: '5'
        type: number
      randomize:
        description: 'Randomize package selection'
        required: false
        default: false
        type: boolean
      model_provider:
        description: 'Model provider to use'
        required: false
        default: 'gemini'
        type: choice
        options:
          - anthropic
          - ollama
          - gemini
          - openai
      model:
        description: 'Model to use (e.g., claude-3-5-haiku-20241022, ollama/qwen2.5-coder:32b, gemini/gemini-2.5-flash, or gpt-4.1-mini-2025-04-14)'
        required: false
        default: 'gemini/gemini-2.5-flash'
        type: string
      ollama_host:
        description: 'Ollama host URL (e.g., http://your-host.tailnet:11434)'
        required: false
        type: string
      dataset:
        description: 'Which dataset to use'
        required: false
        default: 'implementation'
        type: choice
        options:
          - implementation
          - evaluation
      issue_numbers:
        description: 'Comma-separated list of issue numbers to run (overrides dataset, max_packages, and randomize)'
        required: false
        type: string
      timeout_minutes:
        description: 'Timeout for each package build job in minutes (default: 45, vibenix will timeout 15 minutes earlier)'
        required: false
        default: '45'
        type: number
  push:
    branches:
      - main
    paths:
      - 'research/packaging_requests/used_during_implementation.csv'
      - 'src/**'
      - '.github/workflows/train-on-dataset.yml'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Nix
        uses: cachix/install-nix-action@v24
        
      - name: Setup Cachix
        uses: cachix/cachix-action@v15
        with:
          name: vibenix
          authToken: '${{ secrets.CACHIX_TOKEN }}'
        
      - name: Build vibenix environment
        run: |
          nix develop .# --command echo "Development environment ready"
        
      - name: Create matrix from CSV
        id: create-matrix
        run: |
          python3 -c "
          import csv
          import json
          import random
          import sys
          
          matrix_data = []
          
          # Check if specific issue numbers were provided
          issue_numbers_input = '${{ github.event.inputs.issue_numbers }}'.strip()
          
          if issue_numbers_input:
              # When issue numbers are specified, search both datasets
              csv_files = [
                  'research/packaging_requests/used_during_implementation.csv',
                  'research/packaging_requests/post_project_evaluation.csv'
              ]
              all_data = {}
              
              for csv_file in csv_files:
                  with open(csv_file, 'r') as f:
                      reader = csv.DictReader(f)
                      for row in reader:
                          if row['repo_url']:  # Skip entries without repo URLs
                              clean_name = row['repo_url'].replace('https://github.com/', '')
                              all_data[row['issue_number']] = {
                                  'issue_number': row['issue_number'],
                                  'repo_url': row['repo_url'],
                                  'repo_name': clean_name
                              }
          else:
              # Use dataset selection when no specific issues are provided
              dataset = '${{ github.event.inputs.dataset || 'implementation' }}'
              if dataset == 'implementation':
                  csv_file = 'research/packaging_requests/used_during_implementation.csv'
              else:  # evaluation
                  csv_file = 'research/packaging_requests/post_project_evaluation.csv'
              
              # Load data from selected CSV
              all_data = {}
              with open(csv_file, 'r') as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      if row['repo_url']:  # Skip entries without repo URLs
                          clean_name = row['repo_url'].replace('https://github.com/', '')
                          all_data[row['issue_number']] = {
                              'issue_number': row['issue_number'],
                              'repo_url': row['repo_url'],
                              'repo_name': clean_name
                          }
          
          if issue_numbers_input:
              # Process specific issue numbers
              issue_numbers = [num.strip() for num in issue_numbers_input.split(',') if num.strip()]
              missing_issues = []
              
              for issue_num in issue_numbers:
                  if issue_num in all_data:
                      matrix_data.append(all_data[issue_num])
                  else:
                      missing_issues.append(issue_num)
              
              if missing_issues:
                  print(f'ERROR: The following issue numbers were not found in either dataset: {', '.join(missing_issues)}')
                  print(f'Available issue numbers across both datasets: {', '.join(sorted(all_data.keys()))}')
                  sys.exit(1)
          else:
              # Use all data from CSV
              matrix_data = list(all_data.values())
              
              # Randomize if requested
              randomize = '${{ github.event.inputs.randomize }}' == 'true'
              if randomize:
                  random.shuffle(matrix_data)
              
              # Limit by max_packages
              max_packages = int('${{ github.event.inputs.max_packages || 5 }}')
              if max_packages > 0:
                  matrix_data = matrix_data[:max_packages]
          
          # Create matrix
          matrix = {'include': matrix_data}
          
          with open('matrix.json', 'w') as f:
              json.dump(matrix, f)
          
          # Set output for GitHub Actions
          print(f'matrix={json.dumps(matrix)}')
          "
          echo "matrix=$(cat matrix.json)" >> $GITHUB_OUTPUT

  build-test-packages:
    needs: setup
    runs-on: ubuntu-latest
    name: Package ${{ matrix.repo_name }}
    timeout-minutes: ${{ fromJSON(github.event.inputs.timeout_minutes || '45') }}
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      max-parallel: 1  # Run only 1 job at a time to avoid API rate limits
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Nix
        uses: cachix/install-nix-action@v24
        
      - name: Setup Cachix
        uses: cachix/cachix-action@v15
        with:
          name: vibenix
          authToken: '${{ secrets.CACHIX_TOKEN }}'
        
      - name: Tailscale
        if: ${{ (github.event.inputs.model_provider || 'gemini') == 'ollama' }}
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:oauth-client-gh-action-hydralisk-ollama
          
      - name: Create vibenix config
        run: |
          mkdir -p ~/.vibenix
          
          # Convert ollama/modelname to openai/modelname (no-op for other providers)
          model="${{ github.event.inputs.model || 'gemini/gemini-2.5-flash' }}"
          model_converted=$(echo "$model" | sed 's|^ollama/|openai/|')
          
          # Use openai provider if ollama was selected
          provider="${{ github.event.inputs.model_provider || 'gemini' }}"
          if [ "$provider" = "ollama" ]; then
            provider="openai"
          fi
          
          # Build config JSON dynamically
          config_json='{"provider": "'$provider'", "model": "'$model_converted'", "backend": "litellm"'
          
          # Add ollama_host and openai_api_base only if ollama_host input is provided
          if [ -n "${{ github.event.inputs.ollama_host }}" ]; then
            config_json="${config_json}, \"ollama_host\": \"${{ github.event.inputs.ollama_host }}\""
            config_json="${config_json}, \"openai_api_base\": \"${{ github.event.inputs.ollama_host }}/v1\""
          fi
          
          config_json="${config_json}}"
          
          echo "$config_json" > ~/.vibenix/config.json
          
      - name: Setup environment
        run: |
          # Set API keys as environment variables based on provider
          if [ "${{ github.event.inputs.model_provider || 'gemini' }}" = "anthropic" ]; then
            echo "ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}" >> $GITHUB_ENV
          elif [ "${{ github.event.inputs.model_provider || 'gemini' }}" = "gemini" ]; then
            echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" >> $GITHUB_ENV
          elif [ "${{ github.event.inputs.model_provider || 'gemini' }}" = "ollama" ]; then
            # For Ollama via OpenAI endpoint, set a dummy API key if needed
            echo "OPENAI_API_KEY=dummy" >> $GITHUB_ENV
          elif [ "${{ github.event.inputs.model_provider || 'gemini' }}" = "openai" ]; then
            echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV
          fi
      
      - name: Process package request
        id: process
        run: |
          echo "Processing issue #${{ matrix.issue_number }} - ${{ matrix.repo_url }}"
          
          # Set default status
          echo "status=failed" >> $GITHUB_OUTPUT
          
          # Create output directory for this run
          mkdir -p output
          
          # Calculate vibenix timeout (15 minutes less than job timeout)
          job_timeout_minutes=${{ github.event.inputs.timeout_minutes || '45' }}
          vibenix_timeout_minutes=$((job_timeout_minutes - 15))
          vibenix_timeout_seconds=$((vibenix_timeout_minutes * 60))
          
          echo "Job timeout: ${job_timeout_minutes} minutes"
          echo "Vibenix timeout: ${vibenix_timeout_minutes} minutes (${vibenix_timeout_seconds} seconds)"
          
          # Run vibenix with raw output and save results
          set -o pipefail
          VIBENIX_TIMEOUT_SECONDS=${vibenix_timeout_seconds} nix develop .# -c python -m vibenix \
            --raw \
            --output-dir output \
            "${{ matrix.repo_url }}" \
            2>&1 | tee "${{ matrix.issue_number }}.log"
          exit_code=$?
          
          # Check if the command succeeded AND produced a package.nix file
          if [ $exit_code -eq 0 ]; then
            # Command didn't fail, but we need to verify it actually created a package
            if [ -d output ]; then
              # Find any package.nix file in subdirectories
              package_nix_path=$(find output -name "package.nix" -type f | head -n1)
              if [ -n "$package_nix_path" ]; then
                # Extract package name from path (e.g., output/wikiman/package.nix -> wikiman)
                package_name=$(basename $(dirname "$package_nix_path"))
                echo "status=success" >> $GITHUB_OUTPUT
                echo "package_name=$package_name" >> $GITHUB_OUTPUT
              else
                echo "status=failed" >> $GITHUB_OUTPUT
                echo "Command succeeded but no package.nix was generated"
                exit 1  # Fail the step
              fi
            else
              echo "status=failed" >> $GITHUB_OUTPUT
              echo "Command succeeded but no output directory was created"
              exit 1  # Fail the step
            fi
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "Exit code: $exit_code"
            echo "=== Combined log ==="
            head -n 50 "${{ matrix.issue_number }}.log" || echo "No log found"
            exit $exit_code  # Propagate the original exit code
          fi
          
      - name: Prepare artifact directory
        if: always()
        run: |
          mkdir -p artifact-${{ matrix.issue_number }}
          cp ${{ matrix.issue_number }}.log artifact-${{ matrix.issue_number }}/
          if [ -d output ]; then
            cp -r output/* artifact-${{ matrix.issue_number }}/
          fi
          cp ~/.vibenix/config.json artifact-${{ matrix.issue_number }}/
          
          # Create artifact name with sanitized repo name
          repo_name_sanitized=$(echo "${{ matrix.repo_name }}" | tr '/' '-')
          echo "artifact_name=vibenix-output-${{ matrix.issue_number }}-${repo_name_sanitized}" >> $GITHUB_ENV
          
      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.artifact_name }}
          path: artifact-${{ matrix.issue_number }}/
          retention-days: 1
          
      - name: Create summary
        if: always()
        run: |
          # Set emoji based on status
          if [ "${{ steps.process.outputs.status }}" = "success" ]; then
            status_emoji="✅"
            status_text="SUCCESS"
          else
            status_emoji="❌"
            status_text="FAILED"
          fi
          
          echo "## ${status_emoji} Issue #${{ matrix.issue_number }}" >> $GITHUB_STEP_SUMMARY
          echo "- Repository: ${{ matrix.repo_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- Packaging Request: https://github.com/NixOS/nixpkgs/issues/${{ matrix.issue_number }}" >> $GITHUB_STEP_SUMMARY
          echo "- Provider: ${{ github.event.inputs.model_provider || 'gemini' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Model: ${{ github.event.inputs.model || 'gemini/gemini-2.5-flash' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Status: ${status_emoji} **${status_text}**" >> $GITHUB_STEP_SUMMARY
          if [ -n "${{ steps.process.outputs.package_name }}" ]; then
            echo "- Package name: ${{ steps.process.outputs.package_name }}" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "${{ matrix.issue_number }}.log" ]; then
            echo "- Log size: $(wc -c < ${{ matrix.issue_number }}.log) bytes" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "output/${{ steps.process.outputs.package_name }}/package.nix" ]; then
            echo "- Generated package.nix: ✓" >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f "output/run.ccl" ]; then
            echo "- Generated run.ccl: ✓" >> $GITHUB_STEP_SUMMARY
            # Extract total cost from run.ccl if present
            total_cost=$(grep "total_cost =" "output/run.ccl" | tail -1 | cut -d'=' -f2 | tr -d ' ')
            if [ -n "$total_cost" ]; then
              echo "- Total API cost: \$$total_cost" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          if [ -f "${{ matrix.issue_number }}.log" ] && [ -s "${{ matrix.issue_number }}.log" ]; then
            echo "### Log excerpt:" >> $GITHUB_STEP_SUMMARY
            echo '<pre>' >> $GITHUB_STEP_SUMMARY
            tail -n 50 "${{ matrix.issue_number }}.log" >> $GITHUB_STEP_SUMMARY
            echo '</pre>' >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Fail job if packaging failed
        if: steps.process.outputs.status == 'failed'
        run: |
          echo "Packaging failed, failing the job"
          echo "Status was: ${{ steps.process.outputs.status }}"
          exit 1

  aggregate-results:
    needs: build-test-packages
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
          pattern: vibenix-output-*
          
      - name: Aggregate results
        run: |
          echo "# Test Package Build Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          total=0
          success=0
          failed=0
          total_cost=0
          
          # Process each artifact directory
          for dir in artifacts/vibenix-output-*; do
            if [ -d "$dir" ]; then
              # Extract issue number from the directory name (format: vibenix-output-ISSUE-PROJECT)
              base_name=$(basename "$dir")
              issue_num=$(echo "$base_name" | sed 's/vibenix-output-//' | cut -d'-' -f1)
              total=$((total + 1))
              
              # Check if there's a package.nix file in any subdirectory
              package_nix_found=$(find "$dir" -name "package.nix" -type f 2>/dev/null | head -1)
              
              if [ -n "$package_nix_found" ]; then
                success=$((success + 1))
              else
                failed=$((failed + 1))
              fi
              
              # Extract cost from run.ccl if it exists
              if [ -f "$dir/run.ccl" ]; then
                cost=$(grep "total_cost =" "$dir/run.ccl" | tail -1 | cut -d'=' -f2 | tr -d ' ')
                if [ -n "$cost" ]; then
                  # Add to total using bc for floating point arithmetic
                  total_cost=$(echo "$total_cost + $cost" | bc)
                fi
              fi
            fi
          done
          
          echo "## Summary:" >> $GITHUB_STEP_SUMMARY
          echo "- Total processed: $total" >> $GITHUB_STEP_SUMMARY
          echo "- Successful: $success" >> $GITHUB_STEP_SUMMARY
          echo "- Failed: $failed" >> $GITHUB_STEP_SUMMARY
          
          if [ $success -gt 0 ]; then
            success_rate=$((success * 100 / total))
            echo "- Success rate: ${success_rate}%" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Show total cost if any
          if [ $(echo "$total_cost > 0" | bc) -eq 1 ]; then
            echo "- Total API cost: \$$total_cost" >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Create combined artifact
        run: |
          mkdir -p combined-results
          if [ -d artifacts ] && [ "$(ls -A artifacts)" ]; then
            # Process each artifact and reorganize
            for dir in artifacts/vibenix-output-*; do
              if [ -d "$dir" ]; then
                # Extract issue number and project name from the directory name (format: vibenix-output-ISSUE-PROJECT)
                base_name=$(basename "$dir")
                issue_and_project=$(echo "$base_name" | sed 's/vibenix-output-//')
                issue_num=$(echo "$issue_and_project" | cut -d'-' -f1)
                # Get everything after the first dash as the project name
                project_name=$(echo "$issue_and_project" | cut -d'-' -f2-)
                
                # Create folder with issue-project naming
                if [ -n "$project_name" ]; then
                  folder_name="${issue_num}-${project_name}"
                else
                  folder_name="$issue_num"
                fi
                
                # Create issue-specific directory
                mkdir -p "combined-results/$folder_name"
                
                # Copy log file
                if [ -f "$dir/${issue_num}.log" ]; then
                  cp "$dir/${issue_num}.log" "combined-results/$folder_name/"
                fi
                
                # Copy package files (should be flatter now)
                if [ -f "$dir/run.ccl" ]; then
                  cp "$dir/run.ccl" "combined-results/$folder_name/"
                fi
                
                # Copy package directory if it exists
                for pkg_dir in "$dir"/*/; do
                  if [ -d "$pkg_dir" ] && [ -f "$pkg_dir/package.nix" ]; then
                    pkg_name=$(basename "$pkg_dir")
                    mkdir -p "combined-results/$folder_name/$pkg_name"
                    cp "$pkg_dir/package.nix" "combined-results/$folder_name/$pkg_name/"
                  fi
                done
                
                # Copy config.json
                if [ -f "$dir/config.json" ]; then
                  cp "$dir/config.json" "combined-results/$folder_name/"
                fi
              fi
            done
          else
            echo "No artifacts found to combine"
            echo "No successful packages were built" > combined-results/README.txt
          fi
          
      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: all-test-results
          path: combined-results/
          retention-days: 90
